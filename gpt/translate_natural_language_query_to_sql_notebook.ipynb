{"cells":[{"cell_type":"markdown","metadata":{"id":"EV0cfuXJeO-8"},"source":["<a href=\"https://colab.research.google.com/github/ml4devs/ml4devs-notebooks/blob/master/gpt/translate_natural_language_query_to_sql_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e3Id4cfBeO-_"},"source":["<h1><center>Translate Natural Language Queries to SQL with GPT</center></h1>\n","\n","<p><center>\n","<address>&copy; Satish Chandra Gupta<br/>\n","LinkedIn: <a href=\"https://www.linkedin.com/in/scgupta/\">scgupta</a>,\n","Twitter: <a href=\"https://twitter.com/scgupta\">scgupta</a>\n","</address>\n","</center></p>\n","\n","---\n","\n","## Setup Environment\n","\n","### Install Pip Packages\n","\n","You need Python 3.7 or higher to install [OpenAI Python API library](https://github.com/openai/openai-python)."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":770,"status":"ok","timestamp":1701355793267,"user":{"displayName":"","userId":""},"user_tz":-330},"id":"iZkQzDDxeO_A","outputId":"4824a891-78f6-47d5-c78b-5f51c7566483","vscode":{"languageId":"shellscript"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.11.1\n"]}],"source":["# You should have Python 3.7 or higher\n","\n","!python --version\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5781,"status":"ok","timestamp":1701355799502,"user":{"displayName":"","userId":""},"user_tz":-330},"id":"1Y6r5ieIeO_C","outputId":"c6a9c2b8-42a5-4749-d018-df48a804223e","vscode":{"languageId":"shellscript"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai==1.3.6 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (1.3.6)\n","Requirement already satisfied: python-dotenv==1.0.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (1.0.0)\n","Requirement already satisfied: SQLAlchemy==2.0.23 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (2.0.23)\n","Requirement already satisfied: anyio<4,>=3.5.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (1.8.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (0.25.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (2.5.2)\n","Requirement already satisfied: sniffio in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (4.66.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from openai==1.3.6) (4.8.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from SQLAlchemy==2.0.23) (3.0.1)\n","Requirement already satisfied: idna>=2.8 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai==1.3.6) (3.6)\n","Requirement already satisfied: certifi in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.3.6) (2023.11.17)\n","Requirement already satisfied: httpcore==1.* in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.3.6) (1.0.2)\n","Requirement already satisfied: h11<0.15,>=0.13 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.6) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.3.6) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.5 in /Users/scgupta/.pyenv/versions/ml4devs/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai==1.3.6) (2.14.5)\n"]}],"source":["!pip install openai==1.3.6 python-dotenv==1.0.0 SQLAlchemy==2.0.23\n"]},{"cell_type":"markdown","metadata":{"id":"cYfbkKGPeO_D"},"source":["### Upload `.env` File with API Keys\n","\n","You can either use GPT directly from OpenAI, or you can use Azure OpenAI from Microsoft. You need to create a `.env` file and add the environment variables needed for OpenAI api.\n","\n","If you are using OpenAI, check your [OpenAI account](https://platform.openai.com/api-keys) for creating API key. Your `.env` file will look like following:\n","\n","```sh\n","$ cat .env\n","OPENAI_API_KEY='sk-YourOpenAiApiKeyHere'\n","```\n","\n","If you are using Microsoft Azure OpenAI:\n","- Go to [Azure Portal](https://portal.azure.com/) > **All Resources**\n","- Filter the list with Type == Azure OpenAI\n","- Select the one you plan to use\n","- If there are none, you can [create and deploy an Azure OpenAI Service resource](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource)\n","- Click on **Keys and Endpoint** on the left menu\n","- Get `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT`\n","- Next click **Model deployments** on the left menu, and then click **Manage Deployment** button\n","- Alternatively, you can go to [Azure OpenAI Studio](https://oai.azure.com/), and click **Deployments** on the left menu\n","- Find (the latest) API version for [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning)\n","\n","Your `.env` file will look like following:\n","```sh\n","$ cat .env\n","AZURE_OPENAI_API_KEY=yourAzureOpenAiApiKey\n","AZURE_OPENAI_ENDPOINT=https://your-azure-deployment.openai.azure.com/\n","AZURE_OPENAI_DEPLOYMENT_ID=your-deployment-name\n","AZURE_OPENAI_API_VERSION=2023-10-01-preview\n","```\n","\n","Upload `.env` using Upload File button in Google Colab (or Jupyter Notebook). In worst case scenario, uncomment and modify the relevant lines in the following cell to create `.env` file. Please note that it is dangerous to share such notebooks or check them into git."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XKy_X1wdeO_D","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["# Upload or create a .env file with (Azure) OpenAI API creds\n","\n","#!echo \"OPENAI_API_KEY=sk-YourOpenApiKeyHere\" >> .env\n","\n","#!echo \"AZURE_OPENAI_API_KEY=yourAzureOpenAiApiKey\" >> .env\n","#!echo \"AZURE_OPENAI_ENDPOINT=https://your-azure-deployment.openai.azure.com/\" >> .env\n","#!echo \"AZURE_OPENAI_DEPLOYMENT_ID=your-deployment-name\" >> .env\n","#!echo \"AZURE_OPENAI_API_VERSION=2023-10-01-preview\" >> .env\n"]},{"cell_type":"markdown","metadata":{"id":"KcPWLtAWeO_E"},"source":["### Load `.env` File and Specify (Azure) OpenAI GPT Model\n","\n","Load environment variables from `.env` file:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tIT35MxGeO_F"},"outputs":[],"source":["from dotenv import load_dotenv, find_dotenv\n","\n","_ = load_dotenv(find_dotenv())\n"]},{"cell_type":"markdown","metadata":{"id":"cVN8Z7wQeO_F"},"source":["Set `IS_AZURE_OPENAI` flag to `True`, if you are using Azure OpenAI:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KfXGuwQZeO_G"},"outputs":[],"source":["IS_AZURE_OPENAI: bool = False\n"]},{"cell_type":"markdown","metadata":{"id":"2mY86GV2eO_G"},"source":["Specify model name:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GK1r2VFVeO_G"},"outputs":[],"source":["from datetime import datetime\n","\n","GPT35_TURBO: str = \"gpt-3.5-turbo-1106\" if datetime.now() < datetime(2023, 12, 11) else \"gpt-3.5-turbo\"\n","GPT4: str = \"gpt-4\"\n"]},{"cell_type":"markdown","metadata":{"id":"87QWD_kLeO_G"},"source":["---\n","\n","## Setup OpenAI Client with GPT Model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"J3gYa6czeO_H"},"outputs":[],"source":["import os\n","import openai\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yTp0GKx9eO_H"},"outputs":[],"source":["def create_open_ai_client():\n","    if IS_AZURE_OPENAI:\n","        return openai.AzureOpenAI(\n","            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n","            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n","            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n","            azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_ID\")\n","        )\n","    else:\n","        return openai.OpenAI(\n","            api_key=os.getenv('OPENAI_API_KEY')\n","        )\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Hes3w6mfeO_H"},"outputs":[],"source":["openai_client = create_open_ai_client()\n","openai_model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_ID\") if IS_AZURE_OPENAI else GPT4\n","\n","def get_gpt_response(messages, model=openai_model, temperature=0) -> str:\n","    response = openai_client.chat.completions.create(\n","        model=model,\n","        #response_format={\"type\": \"json_object\"},  # Uncomment it if your chosen model supports it\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","    return response.choices[0].message.content\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831,"status":"ok","timestamp":1701355800331,"user":{"displayName":"","userId":""},"user_tz":-330},"id":"5JrKbCv7eO_H","outputId":"0202742f-4049-4f17-f581-9d2508d021c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"message\": \"This is test\"\n","}\n"]}],"source":["print(get_gpt_response([\n","    {\"role\": \"user\", \"content\": \"Say this is test. Format response in JSON\"}\n","]))\n"]},{"cell_type":"markdown","metadata":{"id":"Mu09cyaLeO_I"},"source":["You are all set to use GPT for common NLP tasks such as Sentiment Analysis, Language Translation, Intent/Entity Recognition."]},{"cell_type":"markdown","metadata":{"id":"eOmv1Y7OeO_I"},"source":["---\n","\n","## Setup Database\n","\n","You need a dataset that you will query using natural language. You also need a SQL database that will host that dataset."]},{"cell_type":"markdown","metadata":{},"source":["### Example Dataset: DVD Rental\n","\n","Sakila example dataset is commonly used for teaching and testing RDBMS concept. It has data of fictitious DVD Rental Store. We will use [SQLite](https://www.sqlite.org/index.html) as the database. Python has [sqlite3](https://docs.python.org/3/library/sqlite3.html) package, so it does not require anything to installed and deployed locally or on cloud."]},{"cell_type":"markdown","metadata":{},"source":["1. Download the dataset using `curl` or `wget` command from [SQLite Tutorial](https://www.sqlitetutorial.net/sqlite-sample-database/). Alternatively, you can download from [Kaggle](https://www.kaggle.com/datasets/atanaskanev/sqlite-sakila-sample-database/data) too."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  298k  100  298k    0     0   314k      0 --:--:-- --:--:-- --:--:--  316k\n"]}],"source":["!curl -L0 https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip --output ./chinook.zip\n","\n","#!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\n"]},{"cell_type":"markdown","metadata":{},"source":["2. Unzip the db file"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  chinook.zip\n","  inflating: chinook.db              \n"]}],"source":["!unzip chinook.zip\n"]},{"cell_type":"markdown","metadata":{},"source":["3. The db file will be stored at `./chinook.db`. This is the path you will need when using `sqlite3` package."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r--  1 scgupta  staff  884736 Nov 29  2015 ./chinook.db\n"]}],"source":["!ls -l ./chinook.db\n"]},{"cell_type":"markdown","metadata":{},"source":["4. Extract DB Metadata"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import json\n","import sqlite3\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["DB_FILE_PATH = \"./chinook.db\"\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def extract_sqlite3_db_metadata(sqlite_db_file_path: str):\n","    db_metadata = {}\n","\n","    # Connect to the SQLite database\n","    conn = sqlite3.connect(sqlite_db_file_path)\n","    cursor = conn.cursor()\n","\n","    # Get a list of all tables in the database\n","    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n","    tables = cursor.fetchall()\n","\n","    # Loop through each table and get its columns\n","    for table in tables:\n","        table_name = table[0]\n","        primary_keys = []\n","        foreign_keys = {}\n","        columns_info = {}\n","\n","        # Get table details\n","        cursor.execute(f\"PRAGMA table_info({table_name});\")\n","        columns = cursor.fetchall()\n","\n","        # Extract info about the columns of the current table\n","        for column in columns:\n","            column_name = column[1]\n","            column_type = column[2]\n","            is_primary = (column[5] == 1)\n","\n","            columns_info[column_name] = {\n","                \"type\": column_type,\n","                \"primary\": is_primary,\n","                \"foreign\": {}\n","            }\n","\n","        # Primary Keys\n","        primary_keys = [\n","            c_name\n","            for c_name, c_attrs in columns_info.items()\n","            if c_attrs[\"primary\"] == True\n","        ]\n","\n","        # Get foreign key details\n","        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n","        fk_constraints = cursor.fetchall()\n","\n","        for fk in fk_constraints:\n","            fk_constraint_id = fk[0]\n","            fk_to_table = fk[2]\n","            fk_from_column = fk[3]\n","            fk_to_column = fk[4]\n","\n","            fk_info = {\n","                \"constraint_id\": fk_constraint_id,\n","                \"to_table\": fk_to_table,\n","                \"to_column\": fk_to_column\n","            }\n","            foreign_keys[fk_from_column] = fk_info\n","            columns_info[fk_from_column][\"foreign\"] = fk_info\n","\n","        db_metadata[table_name] = {\n","            \"columns\": columns_info,\n","            \"primary_keys\": primary_keys,\n","            \"foreign_keys\": foreign_keys\n","        }\n","\n","    # Close the connection\n","    conn.close()\n","\n","    # Remove tables with names staring with \"sqlite\" as those are not part of applications\n","    tables_to_remove = [t for t in db_metadata if t.startswith(\"sqlite\")]\n","    for t in tables_to_remove:\n","        del db_metadata[t]\n","\n","    # Done!\n","    return db_metadata\n"]},{"cell_type":"markdown","metadata":{},"source":["5. Check out if the database metadata has been extracted correctly."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def table_info_str(t_name, t_info) -> str:\n","    column_info_str = \"\\n        \".join([\n","        f\"{c_name}: {c_info['type']}\"\n","        for c_name, c_info in t_info[\"columns\"].items()\n","    ])\n","\n","    primary_key_info_str = \"\"\n","    if len(t_info[\"primary_keys\"]) > 0:\n","        primary_key_info_str = f\"Primary Keys: {','.join(t_info['primary_keys'])}\"\n","\n","    foreign_key_info_str = \"\"\n","    if len(t_info[\"foreign_keys\"]) > 0:\n","        foreign_key_info_str = \"\\n    Foreign Keys:\\n        \" + \"\\n        \".join([\n","            f\"{fk_from_col} => {fk_info['to_table']}.{fk_info['to_column']}\"\n","            for fk_from_col, fk_info in t_info[\"foreign_keys\"].items()\n","        ])\n","\n","    return f\"\"\"Table Name: {t_name}\n","    Columns:\n","        {column_info_str}\n","    \"\"\" + primary_key_info_str + foreign_key_info_str\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["chinook_db_metadata = extract_sqlite3_db_metadata(DB_FILE_PATH)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Table Name: albums\n","    Columns:\n","        AlbumId: INTEGER\n","        Title: NVARCHAR(160)\n","        ArtistId: INTEGER\n","    Primary Keys: AlbumId\n","    Foreign Keys:\n","        ArtistId => artists.ArtistId\n","\n","Table Name: artists\n","    Columns:\n","        ArtistId: INTEGER\n","        Name: NVARCHAR(120)\n","    Primary Keys: ArtistId\n","\n","Table Name: customers\n","    Columns:\n","        CustomerId: INTEGER\n","        FirstName: NVARCHAR(40)\n","        LastName: NVARCHAR(20)\n","        Company: NVARCHAR(80)\n","        Address: NVARCHAR(70)\n","        City: NVARCHAR(40)\n","        State: NVARCHAR(40)\n","        Country: NVARCHAR(40)\n","        PostalCode: NVARCHAR(10)\n","        Phone: NVARCHAR(24)\n","        Fax: NVARCHAR(24)\n","        Email: NVARCHAR(60)\n","        SupportRepId: INTEGER\n","    Primary Keys: CustomerId\n","    Foreign Keys:\n","        SupportRepId => employees.EmployeeId\n","\n","Table Name: employees\n","    Columns:\n","        EmployeeId: INTEGER\n","        LastName: NVARCHAR(20)\n","        FirstName: NVARCHAR(20)\n","        Title: NVARCHAR(30)\n","        ReportsTo: INTEGER\n","        BirthDate: DATETIME\n","        HireDate: DATETIME\n","        Address: NVARCHAR(70)\n","        City: NVARCHAR(40)\n","        State: NVARCHAR(40)\n","        Country: NVARCHAR(40)\n","        PostalCode: NVARCHAR(10)\n","        Phone: NVARCHAR(24)\n","        Fax: NVARCHAR(24)\n","        Email: NVARCHAR(60)\n","    Primary Keys: EmployeeId\n","    Foreign Keys:\n","        ReportsTo => employees.EmployeeId\n","\n","Table Name: genres\n","    Columns:\n","        GenreId: INTEGER\n","        Name: NVARCHAR(120)\n","    Primary Keys: GenreId\n","\n","Table Name: invoices\n","    Columns:\n","        InvoiceId: INTEGER\n","        CustomerId: INTEGER\n","        InvoiceDate: DATETIME\n","        BillingAddress: NVARCHAR(70)\n","        BillingCity: NVARCHAR(40)\n","        BillingState: NVARCHAR(40)\n","        BillingCountry: NVARCHAR(40)\n","        BillingPostalCode: NVARCHAR(10)\n","        Total: NUMERIC(10,2)\n","    Primary Keys: InvoiceId\n","    Foreign Keys:\n","        CustomerId => customers.CustomerId\n","\n","Table Name: invoice_items\n","    Columns:\n","        InvoiceLineId: INTEGER\n","        InvoiceId: INTEGER\n","        TrackId: INTEGER\n","        UnitPrice: NUMERIC(10,2)\n","        Quantity: INTEGER\n","    Primary Keys: InvoiceLineId\n","    Foreign Keys:\n","        TrackId => tracks.TrackId\n","        InvoiceId => invoices.InvoiceId\n","\n","Table Name: media_types\n","    Columns:\n","        MediaTypeId: INTEGER\n","        Name: NVARCHAR(120)\n","    Primary Keys: MediaTypeId\n","\n","Table Name: playlists\n","    Columns:\n","        PlaylistId: INTEGER\n","        Name: NVARCHAR(120)\n","    Primary Keys: PlaylistId\n","\n","Table Name: playlist_track\n","    Columns:\n","        PlaylistId: INTEGER\n","        TrackId: INTEGER\n","    Primary Keys: PlaylistId\n","    Foreign Keys:\n","        TrackId => tracks.TrackId\n","        PlaylistId => playlists.PlaylistId\n","\n","Table Name: tracks\n","    Columns:\n","        TrackId: INTEGER\n","        Name: NVARCHAR(200)\n","        AlbumId: INTEGER\n","        MediaTypeId: INTEGER\n","        GenreId: INTEGER\n","        Composer: NVARCHAR(220)\n","        Milliseconds: INTEGER\n","        Bytes: INTEGER\n","        UnitPrice: NUMERIC(10,2)\n","    Primary Keys: TrackId\n","    Foreign Keys:\n","        MediaTypeId => media_types.MediaTypeId\n","        GenreId => genres.GenreId\n","        AlbumId => albums.AlbumId\n","\n"]}],"source":["for t_name, t_info in chinook_db_metadata.items():\n","    print(table_info_str(t_name, t_info))\n","    print()\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Database Table Schema Documents\n","\n","GPT can create a SQL query only if it understands various tables and their columns. While creating the GPT prompt, you must include this info of relevant tables.\n","\n","The `CREATE TABLE` statement of [SQL DDL](https://en.wikipedia.org/wiki/Data_definition_language) captures all necessary info. Ideally, table description and column descriptions should also be captured as comments to assist document search and GPT.\n","\n","Let's create a mapping of table name and their `CREATE TABLE` statements."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def create_table_ddl_stmt_str(t_name, t_info) -> str:\n","    column_defs = \",\\n    \".join([\n","        f\"{c_name} \\t{c_info['type']}\"\n","        for c_name, c_info in t_info[\"columns\"].items()\n","    ])\n","\n","    primary_key_def = \"\"\n","    if len(t_info[\"primary_keys\"]) > 0:\n","        primary_key_def = f\",\\n\\n    PRIMARY KEY ({', '.join(t_info['primary_keys'])})\"\n","\n","    foreign_key_def =\"\"\n","    if len(t_info[\"foreign_keys\"]) > 0:\n","        fk_stmts = \",\\n\".join([\n","            f\"    FOREIGN KEY({fk_from_col}) REFERENCES {fk_info['to_table']}({fk_info['to_column']})\"\n","            for fk_from_col, fk_info in t_info[\"foreign_keys\"].items()\n","        ])\n","        foreign_key_def = f\",\\n\\n{fk_stmts}\"\n","\n","    return f\"\"\"CREATE TABLE {t_name} (\n","    {column_defs}{primary_key_def}{foreign_key_def}\n",");\"\"\"\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's sequence the tables so that the definition of every table referred to in a `FOREIGN KEY` constraint comes before the constraint. While one can write code to analyze foreign key constraint graph and perform a topological sort to get a partial order, I decided to just hand code it as it does not have relevance for this tutorial.\n","\n","You can check the [Entity Relation Model](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model) for all tables drawn using Crow's Foot notation:\n","\n","![](https://www.sqlitetutorial.net/wp-content/uploads/2015/11/sqlite-sample-database-color.jpg)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Table list in Topological Order for foreign key constraints\n","\n","chinook_db_table_names = [\n","    \"artists\", \"albums\",\n","    \"media_types\", \"genres\", \"tracks\",\n","    \"playlists\", \"playlist_track\",\n","    \"employees\",\n","    \"customers\", \"invoices\", \"invoice_items\"\n","]\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["all_chinook_db_table_documents: dict[str, str] = {\n","    t_name: create_table_ddl_stmt_str(t_name, chinook_db_metadata[t_name])\n","    for t_name in chinook_db_table_names\n","}\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["#for t_name in chinook_db_table_names:\n","#    print(all_chinook_db_table_documents[t_name])\n","#    print()\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Natural Language Query to SQL\n","\n","General flow of building applications using Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) has three parts:\n","\n","- **Embeddings**: Data Preprocessing\n","  - Break private data or documents into chunks\n","  - Convert chunks to vectors using an embedding model\n","  - Store vectors in a Vector DB\n","- **Retrieval**: Prompt Construction\n","  - Convert user query into a vector using the same embedding model\n","  - Search the Vector DB for chunk with similar embeddings and rank them\n","  - Craft a prompt using the user query and the document chunks found in the search\n","- **Inference**: Prompt Execution\n","  - Submit the prompt to a LLM\n","  - Post-process (check, augment) the LLM response\n","  - Send the response to the user\n","\n","For converting a natural language query to SQL, RAG pattern will translate to:\n","\n","- Embeddings:\n","  - Consider a `CREATE TABLE` statement for a table as one document chunk\n","  - Convert each `CREATE TABLE` statement to a vector embedding\n","  - Save (embedding, table name) mapping in a Vector DB\n","- Retrieval:\n","  - Convert incoming user query to a vector embedding\n","  - Search Vector DB and find tables with top similarity score\n","  - Craft a prompt using the user query and `CREATE TABLE` statements of all top-matching tables\n","- Inference:\n","  - Submit prompt to GPT to get the equivalent SQL\n","  - Execute the returned SQL on the database\n","  - Present the results to the user\n"]},{"cell_type":"markdown","metadata":{},"source":["### RAG: Vector DB Document Search\n","\n","For sake of simplicity, we will skip the embedding and Vector DB search. Since there are only 11 tables, with not too many columns, we can send DDL for all tables in the prompt."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def find_tables(nl_query: str) -> dict[str, str]:\n","    # Bypassing\n","    # - Convert nl_query => embeddings\n","    # - Search Vector DB for documents (table's CREATE TABLE statement) with similar embeddings\n","    # - Return {table_name: document} mapping for all matching tables\n","    #\n","    # Instead return all documents\n","\n","    return all_chinook_db_table_documents\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prompt Construction\n","\n","Craft a prompt using the user query and the documents returned from Vector DB search"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def nl2sql_system_prompt(documents: dict[str, str], sql_flavor: str = \"Python sqlite3\") -> str:\n","    metadata = \"\\n\".join([\n","        f\"# SQL DDL Schema for `{table_name}` table:```sql\\n{table_schema}```\\n\"\n","        for table_name, table_schema in documents.items()\n","    ])\n","\n","    system_prompt = f\"\"\"\n","    You are a data analyst and data engineer. You are an expert in writing SQL queries\n","    for {sql_flavor} database.\n","\n","    You have following tables in the database. The table name is in single backquote, and\n","    the DDL code to create that table with schema and metadata details are in triple backquote.\n","\n","    ### Database Table Schemas:\n","    \\n{metadata}\n","    ###\n","\n","    User ask you queries in natural language, and you job is to write equivalent\n","    SQL queries in following steps:\n","    1. Identify the tables that have data relevant for the query\n","    2. Identify relevant columns in those tables\n","    3. Craft a SQL query that selects, filters, groups, joins in an optimal order\n","       that is equivalent to the user's natural language query.\n","\n","    Format your response as a JSON dictionary with following key, value:\n","    - tables: a dictionary with the name of relevant tables as keys, and the\n","        list of relevant columns in that as value.\n","    - sql: the sql query that you crafted.\n","    \"\"\"\n","\n","    return system_prompt\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def nl2sql_user_prompt(nl_query: str):\n","    return f\"Write a SQL that computes natural language query in triple backquotes: ```{nl_query}```\"\n"]},{"cell_type":"markdown","metadata":{},"source":["### Prompt Execution"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def write_sql_query(nl_query: str) -> dict:\n","    # Vectorize nl_query and find matching documents (tables and their DDL)\n","    documents = find_tables(nl_query)\n","    # Craft prompt using the natural language queries and matching documents\n","    system_prompt = nl2sql_system_prompt(documents)\n","    user_prompt = nl2sql_user_prompt(nl_query)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_prompt}\n","    ]\n","    response = get_gpt_response(messages)\n","\n","    response_dict = json.loads(response)\n","    return response_dict\n"]},{"cell_type":"markdown","metadata":{},"source":["### Post-processing: Execute SQL"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def execute_sql_query_on_sqlite3(sql_query: str):\n","    conn = sqlite3.connect(DB_FILE_PATH)\n","    cursor = conn.cursor()\n","    result = cursor.execute(sql_query)\n","    rows = result.fetchall()\n","    conn.close()\n","\n","    return rows\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import sqlalchemy\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["sql_engine = sqlalchemy.create_engine(\n","    f\"sqlite:///{os.path.abspath(os.path.join(os.getcwd(), DB_FILE_PATH))}\",\n","    echo=True\n",")\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def execute_sql_query(connection, query):\n","    result_obj = connection.execute(sqlalchemy.text(query))\n","    return result_obj.fetchall()\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def execute_nl_query(nl_query: str):\n","    response = write_sql_query(nl_query)\n","\n","    #response[\"rows\"] = execute_sql_query_on_sqlite3(response[\"sql\"])\n","    with sql_engine.connect() as conn:\n","        response[\"rows\"] = execute_sql_query(conn, response[\"sql\"])\n","\n","    return response\n"]},{"cell_type":"markdown","metadata":{},"source":["### Try"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["test_nl_queries = [\n","    \"Who is the artist with the most albums?\",\n","    \"List the top 3 tracks with maximum sale.\",\n","    \"Name the employee who supports maximum number of customers.\"\n","] \n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-12-01 20:46:01,984 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n","2023-12-01 20:46:01,984 INFO sqlalchemy.engine.Engine SELECT artists.Name, COUNT(albums.AlbumId) as AlbumCount FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId GROUP BY artists.ArtistId ORDER BY AlbumCount DESC LIMIT 1;\n","2023-12-01 20:46:01,985 INFO sqlalchemy.engine.Engine [generated in 0.00112s] ()\n","2023-12-01 20:46:01,986 INFO sqlalchemy.engine.Engine ROLLBACK\n","2023-12-01 20:46:08,690 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n","2023-12-01 20:46:08,690 INFO sqlalchemy.engine.Engine SELECT t.Name, SUM(ii.Quantity) as Total_Sales FROM invoice_items ii JOIN tracks t ON ii.TrackId = t.TrackId GROUP BY ii.TrackId ORDER BY Total_Sales DESC LIMIT 3\n","2023-12-01 20:46:08,691 INFO sqlalchemy.engine.Engine [generated in 0.00118s] ()\n","2023-12-01 20:46:08,694 INFO sqlalchemy.engine.Engine ROLLBACK\n","2023-12-01 20:46:12,770 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n","2023-12-01 20:46:12,770 INFO sqlalchemy.engine.Engine SELECT e.FirstName, e.LastName FROM employees e WHERE e.EmployeeId = (SELECT c.SupportRepId FROM customers c GROUP BY c.SupportRepId ORDER BY COUNT(*) DESC LIMIT 1)\n","2023-12-01 20:46:12,770 INFO sqlalchemy.engine.Engine [generated in 0.00090s] ()\n","2023-12-01 20:46:12,771 INFO sqlalchemy.engine.Engine ROLLBACK\n"]}],"source":["results = []\n","for nl_q in test_nl_queries:\n","    response =  execute_nl_query(nl_q)\n","    response[\"query\"] = nl_q\n","    results.append(response)\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User Query: Who is the artist with the most albums?\n","Tables:{\"artists\": [\"ArtistId\", \"Name\"], \"albums\": [\"ArtistId\"]}\n","SQL:SELECT artists.Name, COUNT(albums.AlbumId) as AlbumCount FROM artists JOIN albums ON artists.ArtistId = albums.ArtistId GROUP BY artists.ArtistId ORDER BY AlbumCount DESC LIMIT 1;\n","Rows:\n","[('Iron Maiden', 21)]\n","\n","User Query: List the top 3 tracks with maximum sale.\n","Tables:{\"invoice_items\": [\"TrackId\", \"Quantity\"], \"tracks\": [\"TrackId\", \"Name\"]}\n","SQL:SELECT t.Name, SUM(ii.Quantity) as Total_Sales FROM invoice_items ii JOIN tracks t ON ii.TrackId = t.TrackId GROUP BY ii.TrackId ORDER BY Total_Sales DESC LIMIT 3\n","Rows:\n","[('Balls to the Wall', 2), ('Inject The Venom', 2), ('Snowballed', 2)]\n","\n","User Query: Name the employee who supports maximum number of customers.\n","Tables:{\"employees\": [\"EmployeeId\", \"FirstName\", \"LastName\"], \"customers\": [\"SupportRepId\"]}\n","SQL:SELECT e.FirstName, e.LastName FROM employees e WHERE e.EmployeeId = (SELECT c.SupportRepId FROM customers c GROUP BY c.SupportRepId ORDER BY COUNT(*) DESC LIMIT 1)\n","Rows:\n","[('Jane', 'Peacock')]\n","\n"]}],"source":["for t in results:\n","    print(f\"User Query: {t['query']}\\nTables:{json.dumps(t['tables'])}\\nSQL:{t['sql']}\\nRows:\\n{str(t['rows'])}\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Cleanup"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["sql_engine.dispose()\n"]},{"cell_type":"markdown","metadata":{"id":"-_oyoZAkeO_L"},"source":["---\n","<p>Copyright &copy 2023 <a href=\"https://www.linkedin.com/in/scgupta\">Satish Chandra Gupta</a>.</p>\n","<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\" align=\"left\"/> <p>&nbsp;<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\">CC BY-NC-SA 4.0 International</a> License.</p>"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ml4devs/ml4devs-notebooks/blob/master/gpt/nlp_with_gpt_notebook.ipynb","timestamp":1701355922767},{"file_id":"https://github.com/ml4devs/ml4devs-notebooks/blob/master/gpt/nlp_with_gpt_notebook.ipynb","timestamp":1701352821148}]},"kernelspec":{"display_name":"ml4devs","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
