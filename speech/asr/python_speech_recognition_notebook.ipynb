{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_speech_recognition_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOugDdFcqekLlzo1yrW03ry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scgupta/ml4devs-notebooks/blob/master/speech/asr/python_speech_recognition_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBo4UlPaq4O"
      },
      "source": [
        "<h1><center>Speech Recognition with Python</center></h1>\n",
        "\n",
        "<p><center>\n",
        "<address>&copy; Satish Chandra Gupta<br/>\n",
        "LinkedIn: <a href=\"https://www.linkedin.com/in/scgupta/\">scgupta</a>,\n",
        "Twitter: <a href=\"https://twitter.com/scgupta\">scgupta</a>\n",
        "</address> \n",
        "</center></p>\n",
        "\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Blog Post: [Speech Recognition With Python](https://www.ml4devs.com/articles/speech-recognition-with-python/)\n",
        "\n",
        "There are several Automated Speech Recognition (ASR) alternatives, and most of them have bindings for Python. There are two kinds of solutions:\n",
        "\n",
        "- **Service:** These run on the cloud, and are accessed either through REST endpoints or Python library. Examples are cloud speech services from Google, Amazon, Microsoft.\n",
        "- **Software:** These run locally on the machine (not requiring network connection). Examples are CMU Sphinx and Mozilla DeepSpeech.\n",
        "\n",
        "Speech Recognition APIs are of two types:\n",
        "- **Batch:** The full audio file is passed as parameter, and speech-to-text transcribing is done in one shot.\n",
        "- **Streaming:** The chunks of audio buffer are repeatedly passed on, and intermediate results are accessible.\n",
        "\n",
        "All packages support batch mode, and some support streaming mode too.\n",
        "\n",
        "One common use case is to collect audio from microphone and passes on the buffer to the speech recognition API. Invariably, in such transcribers, microphone is accessed though [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/), which is implemented over [PortAudio](http://www.portaudio.com/).\n",
        "\n",
        "From Colab menu, select: **Runtime** > **Change runtime type**, and verify that it is set to Python3, and select GPU if you want to try out GPU version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H6HFpi_LCBt"
      },
      "source": [
        "## Common Setup\n",
        "\n",
        "1. **Install google cloud speech package**\n",
        "\n",
        "You may have to restart the runtime after this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhIMRXQPLaaA",
        "outputId": "ee11bcc4-44ab-4f96-ec5e-84d5b2f630b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "!pip3 install google-cloud-speech"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-speech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/81/c59a373c7668beb9de922b9c4419b793898d46c6d4a44f4fe28098e77623/google_cloud_speech-1.3.1-py2.py3-none-any.whl (88kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 33.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 61kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-speech) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.6.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (3.10.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (42.0.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2018.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.12.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.8.2; extra == \"grpc\" in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.15.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2019.11.28)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (0.2.7)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (4.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (0.4.8)\n",
            "Installing collected packages: google-cloud-speech\n",
            "Successfully installed google-cloud-speech-1.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjuKePpRJt5"
      },
      "source": [
        "2. **Download audio files for testing**\n",
        "\n",
        "Following files will be used as test cases for all speech recognition alternatives covered in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3_2z6qMRXcY",
        "outputId": "c155e814-439b-4fb0-d26f-307516b3ab97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/audio-0.6.0.tar.gz\n",
        "!tar -xvzf audio-0.6.0.tar.gz\n",
        "!ls -l ./audio/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   608    0   608    0     0   2632      0 --:--:-- --:--:-- --:--:--  2620\n",
            "100  192k  100  192k    0     0   310k      0 --:--:-- --:--:-- --:--:--  310k\n",
            "audio/\n",
            "audio/2830-3980-0043.wav\n",
            "audio/Attribution.txt\n",
            "audio/4507-16021-0012.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/License.txt\n",
            "total 260\n",
            "-rw-r--r-- 1 501 staff 63244 Nov 18  2017 2830-3980-0043.wav\n",
            "-rw-r--r-- 1 501 staff 87564 Nov 18  2017 4507-16021-0012.wav\n",
            "-rw-r--r-- 1 501 staff 82924 Nov 18  2017 8455-210777-0068.wav\n",
            "-rw-r--r-- 1 501 staff   340 May 14  2018 Attribution.txt\n",
            "-rw-r--r-- 1 501 staff 18652 May 12  2018 License.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXzH4pu9Kxr4"
      },
      "source": [
        "3. **Define test cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbzYo01kRi8P"
      },
      "source": [
        "TESTCASES = [\n",
        "  {\n",
        "    'filename': 'audio/2830-3980-0043.wav',\n",
        "    'text': 'experience proves this',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  },\n",
        "  {\n",
        "    'filename': 'audio/4507-16021-0012.wav',\n",
        "    'text': 'why should one halt on the way',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  },\n",
        "  {\n",
        "    'filename': 'audio/8455-210777-0068.wav',\n",
        "    'text': 'your power is sufficient i said',\n",
        "    'encoding': 'LINEAR16',\n",
        "    'lang': 'en-US'\n",
        "  }\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-23iCNLvBIx"
      },
      "source": [
        "4. **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujeuvj35Ksv8"
      },
      "source": [
        "from typing import Tuple\n",
        "import wave\n",
        "\n",
        "def read_wav_file(filename) -> Tuple[bytes, int]:\n",
        "    with wave.open(filename, 'rb') as w:\n",
        "        rate = w.getframerate()\n",
        "        frames = w.getnframes()\n",
        "        buffer = w.readframes(frames)\n",
        "\n",
        "    return buffer, rate\n",
        "\n",
        "def simulate_stream(buffer: bytes, batch_size: int = 4096):\n",
        "    buffer_len = len(buffer)\n",
        "    offset = 0\n",
        "    while offset < buffer_len:\n",
        "        end_offset = offset + batch_size\n",
        "        buf = buffer[offset:end_offset]\n",
        "        yield buf\n",
        "        offset = end_offset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wFdQoEUQH-3"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Google\n",
        "\n",
        "Google has [speech-to-text](https://cloud.google.com/speech-to-text/docs) as one of the Google Cloud services. It has [libraries](https://cloud.google.com/speech-to-text/docs/reference/libraries) in C#, Go, Java, JavaScript, PHP, Python, and Ruby. It supports both batch and stream modes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjcnbQvvY3Xu"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Upload Google Cloud Cred file**\n",
        "\n",
        "Have Google Cloud creds stored in a file named **`gc-creds.json`**, and upload it by running following code cell. See https://developers.google.com/accounts/docs/application-default-credentials for more details.\n",
        "\n",
        "This may reqire enabling **third-party cookies**. Check out https://colab.research.google.com/notebooks/io.ipynb for other alternatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXquL3Y7bLQ6",
        "outputId": "5a46b013-8b39-4227-ba5b-28677885d763",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-181e99b1-b88d-4bab-9b36-9db9bfb86b94\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-181e99b1-b88d-4bab-9b36-9db9bfb86b94\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving gc-creds.json to gc-creds.json\n",
            "User uploaded file \"gc-creds.json\" with length 2314 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emXbdQ1bTDg",
        "outputId": "2a343cc2-8a58-476c-ee89-8717954a08dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pwd\n",
        "!ls -l ./gc-creds.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "-rw-r--r-- 1 root root 2314 Jan 30 00:20 ./gc-creds.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-80JnSyMWEV"
      },
      "source": [
        "2. **Set environment variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msFTMyUWgtEv",
        "outputId": "ead40fcc-e41f-4ff6-e79d-418aad093c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/gc-creds.json'\n",
        "\n",
        "!ls -l $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 2314 Jan 30 00:20 /content/gc-creds.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBsBFHWCMi"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlm4CWyQPeR",
        "outputId": "d5bfd6f5-e16a-4944-ad36-4325f9021935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from google.cloud import speech_v1\n",
        "from google.cloud.speech_v1 import enums\n",
        "\n",
        "def google_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    client = speech_v1.SpeechClient()\n",
        "\n",
        "    config = {\n",
        "        'language_code': lang,\n",
        "        'sample_rate_hertz': rate,\n",
        "        'encoding': enums.RecognitionConfig.AudioEncoding[encoding]\n",
        "    }\n",
        "\n",
        "    audio = {\n",
        "        'content': buffer\n",
        "    }\n",
        "\n",
        "    response = client.recognize(config, audio)\n",
        "    # For bigger audio file, the previous line can be replaced with following:\n",
        "    # operation = client.long_running_recognize(config, audio)\n",
        "    # response = operation.result()\n",
        "\n",
        "    for result in response.results:\n",
        "        # First alternative is the most probable result\n",
        "        alternative = result.alternatives[0]\n",
        "        return alternative.transcript\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('google-cloud-batch-stt: \"{}\"'.format(\n",
        "        google_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "google-cloud-batch-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "google-cloud-batch-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "google-cloud-batch-stt: \"your power is sufficient I said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhaFWC7rN9b"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9wMydkzrdX-",
        "outputId": "53c3427b-6a1e-4520-939b-a9bc34515760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "from google.cloud import speech\n",
        "from google.cloud.speech import enums\n",
        "from google.cloud.speech import types\n",
        "\n",
        "def response_stream_processor(responses):\n",
        "    print('interim results: ')\n",
        "\n",
        "    transcript = ''\n",
        "    num_chars_printed = 0\n",
        "    for response in responses:\n",
        "        if not response.results:\n",
        "            continue\n",
        "\n",
        "        result = response.results[0]\n",
        "        if not result.alternatives:\n",
        "            continue\n",
        "\n",
        "        transcript = result.alternatives[0].transcript\n",
        "        print('{0}final: {1}'.format(\n",
        "            '' if result.is_final else 'not ',\n",
        "            transcript\n",
        "        ))\n",
        "\n",
        "    return transcript\n",
        "\n",
        "def google_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    config = types.RecognitionConfig(\n",
        "        encoding=enums.RecognitionConfig.AudioEncoding[encoding],\n",
        "        sample_rate_hertz=rate,\n",
        "        language_code=lang\n",
        "    )\n",
        "\n",
        "    streaming_config = types.StreamingRecognitionConfig(\n",
        "        config=config,\n",
        "        interim_results=True\n",
        "    )\n",
        "\n",
        "    audio_generator = simulate_stream(buffer)  # buffer chunk generator\n",
        "    requests = (types.StreamingRecognizeRequest(audio_content=chunk) for chunk in audio_generator)\n",
        "    responses = client.streaming_recognize(streaming_config, requests)\n",
        "    # Now, put the transcription responses to use.\n",
        "    return response_stream_processor(responses)\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('google-cloud-streaming-stt: \"{}\"'.format(\n",
        "        google_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "interim results: \n",
            "not final: next\n",
            "not final: iSpy\n",
            "not final: Aspira\n",
            "not final: Xperia\n",
            "not final: Experian\n",
            "not final: experience\n",
            "not final: experience proved\n",
            "not final: experience proves\n",
            "not final: experience proves the\n",
            "not final: experience proves that\n",
            "not final: experience\n",
            "final: experience proves this\n",
            "google-cloud-streaming-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "interim results: \n",
            "not final: what\n",
            "not final: watch\n",
            "not final: why should\n",
            "not final: why should we\n",
            "not final: why should one\n",
            "not final: why should one who\n",
            "not final: why should one have\n",
            "not final: why should\n",
            "not final: why should\n",
            "not final: why should\n",
            "not final: why should\n",
            "not final: why should one\n",
            "not final: why should one\n",
            "not final: why should one\n",
            "not final: why should one\n",
            "not final: why should one halt\n",
            "not final: why should one halt on\n",
            "not final: why should one halt on the\n",
            "final: why should one halt on the way\n",
            "google-cloud-streaming-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "interim results: \n",
            "not final: you're\n",
            "not final: your pie\n",
            "not final: your power\n",
            "not final: your power is\n",
            "not final: your power is so\n",
            "not final: your power is a\n",
            "not final: your\n",
            "not final: your power\n",
            "not final: your power\n",
            "not final: your power is\n",
            "not final: your power is\n",
            "not final: your power is\n",
            "not final: your power is\n",
            "not final: your power is sufficient\n",
            "final: your power is sufficient I said\n",
            "google-cloud-streaming-stt: \"your power is sufficient I said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Fg2BE75Qoo"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Microsoft Azure\n",
        "\n",
        "Microsoft Azure [Speech Services](https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/) have [Speech to Text](https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/) service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk8NgzQIlwwX"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install azure speech package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A5YJHlswQSs",
        "outputId": "4f2e7f65-dfcc-450e-cb26-c9ca780957c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip3 install azure-cognitiveservices-speech"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-cognitiveservices-speech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/d8/690896a3543b7bed058029b1b3450f4ce2e952d19347663fe570e6dec72c/azure_cognitiveservices_speech-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ME2jnAimEQ"
      },
      "source": [
        "2. **Set service credentials**\n",
        "\n",
        "You can enable Speech service and find credentials for your account at [Microsoft Azure portal](https://portal.azure.com/). You can open a free account [here](https://azure.microsoft.com/en-in/free/ai/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSqzFx-lwyz7"
      },
      "source": [
        "AZURE_SPEECH_KEY = 'YOUR AZURE SPEECH KEY'\n",
        "AZURE_SERVICE_REGION = 'YOUR AZURE SERVICE REGION'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVvMt_qylUjF"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRMjNB68wYYN",
        "outputId": "7539fb6e-1625-4931-f981-96874628c934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def azure_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    speech_config = speechsdk.SpeechConfig(\n",
        "        subscription=AZURE_SPEECH_KEY,\n",
        "        region=AZURE_SERVICE_REGION\n",
        "    )\n",
        "    audio_input = speechsdk.AudioConfig(filename=filename)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
        "        speech_config=speech_config,\n",
        "        audio_config=audio_input\n",
        "    )\n",
        "    result = speech_recognizer.recognize_once()\n",
        "\n",
        "    return result.text if result.reason == speechsdk.ResultReason.RecognizedSpeech else None\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('azure-batch-stt: \"{}\"'.format(\n",
        "        azure_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "azure-batch-stt: \"Experience proves this.\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "azure-batch-stt: \"Whi should one halt on the way.\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "azure-batch-stt: \"Your power is sufficient I said.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXd7OC7plbAu"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzfBW4kczY9l",
        "outputId": "8694efbf-5886-4359-ec6c-5b6f3c970372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import time\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def azure_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    speech_config = speechsdk.SpeechConfig(\n",
        "        subscription=AZURE_SPEECH_KEY,\n",
        "        region=AZURE_SERVICE_REGION\n",
        "    )\n",
        "    stream = speechsdk.audio.PushAudioInputStream()\n",
        "    audio_config = speechsdk.audio.AudioConfig(stream=stream)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
        "        speech_config=speech_config,\n",
        "        audio_config=audio_config\n",
        "    )\n",
        "\n",
        "    # Connect callbacks to the events fired by the speech recognizer\n",
        "    speech_recognizer.recognizing.connect(lambda evt: print('interim text: \"{}\"'.format(evt.result.text)))\n",
        "    speech_recognizer.recognized.connect(lambda evt:  print('azure-streaming-stt: \"{}\"'.format(evt.result.text)))\n",
        "\n",
        "    # start continuous speech recognition\n",
        "    speech_recognizer.start_continuous_recognition()\n",
        "\n",
        "    # push buffer chunks to stream\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "    for chunk in audio_generator:\n",
        "      stream.write(chunk)\n",
        "      time.sleep(0.1)  # to give callback a chance against this fast loop\n",
        "\n",
        "    # stop continuous speech recognition\n",
        "    stream.close()\n",
        "    time.sleep(0.5)  # give chance to VAD to kick in\n",
        "    speech_recognizer.stop_continuous_recognition()\n",
        "    time.sleep(0.5)  # Let all callback run\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    azure_streaming_stt(t['filename'], t['lang'], t['encoding'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "interim text: \"experience\"\n",
            "interim text: \"experienced\"\n",
            "interim text: \"experience\"\n",
            "interim text: \"experience proves\"\n",
            "interim text: \"experience proves this\"\n",
            "azure-streaming-stt: \"Experience proves this.\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "interim text: \"huaisheng\"\n",
            "interim text: \"white\"\n",
            "interim text: \"whi should\"\n",
            "interim text: \"whi should one\"\n",
            "interim text: \"whi should one halt\"\n",
            "interim text: \"whi should one halt on\"\n",
            "interim text: \"whi should one halt on the\"\n",
            "interim text: \"whi should one halt on the way\"\n",
            "azure-streaming-stt: \"Whi should one halt on the way.\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "interim text: \"you're\"\n",
            "interim text: \"your\"\n",
            "interim text: \"your power\"\n",
            "interim text: \"your\"\n",
            "interim text: \"your power is\"\n",
            "interim text: \"your power is sufficient\"\n",
            "interim text: \"your power is sufficient i\"\n",
            "interim text: \"your power is sufficient i said\"\n",
            "azure-streaming-stt: \"Your power is sufficient I said.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASpAymRMzOz"
      },
      "source": [
        "---\n",
        "\n",
        "# IBM Watson\n",
        "\n",
        "For IBM [Watson Speech to Text](https://www.ibm.com/in-en/cloud/watson-speech-to-text) is ASR service with .NET, Go, JavaScript, [Python](https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text?code=python), Ruby, Swift and Unity API libraries, as well as REST endpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atuGghM2RxWd"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install IBM Watson package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5jW68yRWGk",
        "outputId": "b3beae11-4775-430c-8f5b-57ad5809bd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "!pip install ibm-watson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm-watson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/f4/7e256026ee22c75a630c6de53eb45b6fef4840ac6728b80a92dd2e523a1a/ibm-watson-4.2.1.tar.gz (348kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from ibm-watson) (2.21.0)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from ibm-watson) (2.6.1)\n",
            "Collecting websocket-client==0.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.6MB/s \n",
            "\u001b[?25hCollecting ibm_cloud_sdk_core==1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/f6/10d5271c807d73d236e6ae07b68035fed78b28b5ab836704d34097af3986/ibm-cloud-sdk-core-1.5.1.tar.gz\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python_dateutil>=2.5.3->ibm-watson) (1.12.0)\n",
            "Collecting PyJWT>=1.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-4.2.1-cp36-none-any.whl size=343298 sha256=3fcdea1185ceb522ed5f080ad4d66048d9286cd28e8d9bc86094b08a84cb6211\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/4d/6e/ae352b7c7acdddf073aeb06617fbfeefaea9fcb6d7ae98800b\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-1.5.1-cp36-none-any.whl size=44492 sha256=8fbd5fdfa4ca15217877ee44671387c23ce61f390cd15d8006200d502d56dc63\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/42/50/f96888116b329578304f9dda4693cef6f3e76e18272d22cb6c\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson\n",
            "Successfully installed PyJWT-1.7.1 ibm-cloud-sdk-core-1.5.1 ibm-watson-4.2.1 websocket-client-0.48.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bntnwqJ3Q99Z"
      },
      "source": [
        "2. **Set service credentials**\n",
        "\n",
        "You will need to [sign up/in](https://cloud.ibm.com/docs/services/text-to-speech?topic=text-to-speech-gettingStarted), and get API key credential and service URL, and fill it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdl6Y7MJPtoT"
      },
      "source": [
        "WATSON_API_KEY = 'YOUR WATSON API KEY'\n",
        "WATSON_STT_URL = 'YOUR WATSON SERVICE URL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jqxI2XrRmKz"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFWX40PYRogi",
        "outputId": "6feb0b03-8ca4-485e-bf25-5042340f8ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import os\n",
        "\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "def watson_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    authenticator = IAMAuthenticator(WATSON_API_KEY)\n",
        "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
        "    speech_to_text.set_service_url(WATSON_STT_URL)\n",
        "\n",
        "    with open(filename, 'rb') as audio_file:\n",
        "        response = speech_to_text.recognize(\n",
        "            audio=audio_file,\n",
        "            content_type='audio/{}'.format(os.path.splitext(filename)[1][1:]),\n",
        "            model=lang + '_BroadbandModel',\n",
        "            max_alternatives=3,\n",
        "        ).get_result()\n",
        "\n",
        "    return response['results'][0]['alternatives'][0]['transcript']\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('watson-batch-stt: \"{}\"'.format(\n",
        "        watson_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "watson-batch-stt: \"experience proves this \"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "watson-batch-stt: \"why should one hold on the way \"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "watson-batch-stt: \"your power is sufficient I set \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsBdku-RpB-"
      },
      "source": [
        "## Streaming API\n",
        "\n",
        "Streaming API works over websocket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwb0uZjPR0rX",
        "outputId": "97e95e3f-053e-46b7-c459-13697a6eb872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "# Watson websocket prints justs too many debug logs, so disable it\n",
        "logging.disable(logging.CRITICAL)\n",
        "\n",
        "# Chunk and buffer size\n",
        "CHUNK_SIZE = 4096\n",
        "BUFFER_MAX_ELEMENT = 10\n",
        "\n",
        "# A callback class to process various streaming STT events\n",
        "class MyRecognizeCallback(RecognizeCallback):\n",
        "    def __init__(self):\n",
        "        RecognizeCallback.__init__(self)\n",
        "        self.transcript = None\n",
        "\n",
        "    def on_transcription(self, transcript):\n",
        "        # print('transcript: {}'.format(transcript))\n",
        "        pass\n",
        "\n",
        "    def on_connected(self):\n",
        "        # print('Connection was successful')\n",
        "        pass\n",
        "\n",
        "    def on_error(self, error):\n",
        "        # print('Error received: {}'.format(error))\n",
        "        pass\n",
        "\n",
        "    def on_inactivity_timeout(self, error):\n",
        "        # print('Inactivity timeout: {}'.format(error))\n",
        "        pass\n",
        "\n",
        "    def on_listening(self):\n",
        "        # print('Service is listening')\n",
        "        pass\n",
        "\n",
        "    def on_hypothesis(self, hypothesis):\n",
        "        # print('hypothesis: {}'.format(hypothesis))\n",
        "        pass\n",
        "\n",
        "    def on_data(self, data):\n",
        "        self.transcript = data['results'][0]['alternatives'][0]['transcript']\n",
        "        print('{0}final: {1}'.format(\n",
        "            '' if data['results'][0]['final'] else 'not ',\n",
        "            self.transcript\n",
        "        ))\n",
        "\n",
        "    def on_close(self):\n",
        "        # print(\"Connection closed\")\n",
        "        pass\n",
        "\n",
        "def watson_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    authenticator = IAMAuthenticator(WATSON_API_KEY)\n",
        "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
        "    speech_to_text.set_service_url(WATSON_STT_URL)\n",
        "\n",
        "    # Make watson audio source fed by a buffer queue\n",
        "    buffer_queue = Queue(maxsize=BUFFER_MAX_ELEMENT)\n",
        "    audio_source = AudioSource(buffer_queue, True, True)\n",
        "\n",
        "    # Callback object\n",
        "    mycallback = MyRecognizeCallback()\n",
        "\n",
        "    # Read the file\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "\n",
        "    # Start Speech-to-Text recognition thread\n",
        "    stt_stream_thread = Thread(\n",
        "        target=speech_to_text.recognize_using_websocket,\n",
        "        kwargs={\n",
        "            'audio': audio_source,\n",
        "            'content_type': 'audio/l16; rate={}'.format(rate),\n",
        "            'recognize_callback': mycallback,\n",
        "            'interim_results': True\n",
        "        }\n",
        "    )\n",
        "    stt_stream_thread.start()\n",
        "\n",
        "    # Simulation audio stream by breaking file into chunks and filling buffer queue\n",
        "    audio_generator = simulate_stream(buffer, CHUNK_SIZE)\n",
        "    for chunk in audio_generator:\n",
        "        buffer_queue.put(chunk)\n",
        "        time.sleep(0.5)  # give a chance to callback\n",
        "\n",
        "    # Close the audio feed and wait for STTT thread to complete\n",
        "    audio_source.completed_recording()\n",
        "    stt_stream_thread.join()\n",
        "\n",
        "    # send final result\n",
        "    return mycallback.transcript\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('watson-cloud-streaming-stt: \"{}\"'.format(\n",
        "        watson_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "not final: X. \n",
            "not final: experts \n",
            "not final: experience \n",
            "not final: experienced \n",
            "not final: experience prove \n",
            "not final: experience proves \n",
            "not final: experience proves that \n",
            "not final: experience proves this \n",
            "final: experience proves this \n",
            "watson-cloud-streaming-stt: \"experience proves this \"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "not final: why \n",
            "not final: what \n",
            "not final: why should \n",
            "not final: why should we \n",
            "not final: why should one \n",
            "not final: why should one whole \n",
            "not final: why should one hold \n",
            "not final: why should one hold on \n",
            "not final: why should one hold on the \n",
            "not final: why should one hold on the way \n",
            "final: why should one hold on the way \n",
            "watson-cloud-streaming-stt: \"why should one hold on the way \"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "not final: your \n",
            "not final: your power \n",
            "not final: your power is \n",
            "not final: your power is the \n",
            "not final: your power is sufficient \n",
            "not final: your power is sufficient I \n",
            "not final: your power is sufficient I saw \n",
            "not final: your power is sufficient I said \n",
            "not final: your power is sufficient I set \n",
            "final: your power is sufficient I set \n",
            "watson-cloud-streaming-stt: \"your power is sufficient I set \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7W8nsP45IUx"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# CMU Sphinx\n",
        "\n",
        "[CMUSphinx](https://cmusphinx.github.io/) is has been around for quite some time, and has been adapting to advancements in ASR technologies. [PocketSphinx](https://github.com/cmusphinx/pocketsphinx-python) is speech-to-text decoder software package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSdcTyoTl3XL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install swig**\n",
        "\n",
        "For macOS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWqogsSSk2H"
      },
      "source": [
        "!brew install swig\n",
        "!swig -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn_LqRxjSoMT"
      },
      "source": [
        "For Linux:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID2AUZX4SqkU",
        "outputId": "01770ecb-4b8d-4047-c1a2-2f1490c2f74b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "!apt-get install -y swig libpulse-dev\n",
        "!swig -version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpulse-mainloop-glib0 swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  libpulse-dev libpulse-mainloop-glib0 swig swig3.0\n",
            "0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,204 kB of archives.\n",
            "After this operation, 6,538 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.4 [22.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.4 [81.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,204 kB in 1s (1,336 kB/s)\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.4_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../libpulse-dev_1%3a11.1-1ubuntu7.4_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "\n",
            "SWIG Version 3.0.12\n",
            "\n",
            "Compiled with g++ [x86_64-pc-linux-gnu]\n",
            "\n",
            "Configured options: +pcre\n",
            "\n",
            "Please see http://www.swig.org for reporting bugs and further information\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZlvnjsfSu-3"
      },
      "source": [
        "2. **Install poocketsphinx using pip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOkKfgKS789",
        "outputId": "116c7a60-cf24-4ba9-f3ac-ab09a5da0145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!pip3 install pocketsphinx\n",
        "!pip3 list | grep pocketsphinx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pocketsphinx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/4a/adea55f189a81aed88efa0b0e1d25628e5ed22622ab9174bf696dd4f9474/pocketsphinx-0.1.15.tar.gz (29.1MB)\n",
            "\u001b[K     |████████████████████████████████| 29.1MB 102kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pocketsphinx\n",
            "  Building wheel for pocketsphinx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pocketsphinx: filename=pocketsphinx-0.1.15-cp36-cp36m-linux_x86_64.whl size=30126870 sha256=d111bc1a768251e9b8b4bea71f05b498955eda209f5d5650f7e68cc336bb5075\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/fd/52/2f62c9a0036940cc0c89e58ee0b9d00fcf78243aeaf416265f\n",
            "Successfully built pocketsphinx\n",
            "Installing collected packages: pocketsphinx\n",
            "Successfully installed pocketsphinx-0.1.15\n",
            "pocketsphinx                   0.1.15     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weYD8oA-S-vu"
      },
      "source": [
        "## Create Decoder object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEpNoVUiTK4k"
      },
      "source": [
        "import pocketsphinx\n",
        "import os\n",
        "\n",
        "MODELDIR = os.path.join(os.path.dirname(pocketsphinx.__file__), 'model')\n",
        "\n",
        "config = pocketsphinx.Decoder.default_config()\n",
        "config.set_string('-hmm', os.path.join(MODELDIR, 'en-us'))\n",
        "config.set_string('-lm', os.path.join(MODELDIR, 'en-us.lm.bin'))\n",
        "config.set_string('-dict', os.path.join(MODELDIR, 'cmudict-en-us.dict'))\n",
        "\n",
        "decoder = pocketsphinx.Decoder(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6s9CCA9WvIZ"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klStePTBWxO7",
        "outputId": "c06996f3-5da9-4b1a-a5bc-a18c584da3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def sphinx_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    decoder.start_utt()\n",
        "    decoder.process_raw(buffer, False, False)\n",
        "    decoder.end_utt()\n",
        "    hypothesis = decoder.hyp()\n",
        "    return hypothesis.hypstr\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('sphinx-batch-stt: \"{}\"'.format(\n",
        "        sphinx_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx-batch-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx-batch-stt: \"why should one hold on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx-batch-stt: \"your paris sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQJ82tkTTyX3"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfmRd6qTzq9",
        "outputId": "65acfc30-d65e-432b-8abe-ed101ae4ee00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def sphinx_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "\n",
        "    decoder.start_utt()\n",
        "    for chunk in audio_generator:\n",
        "        decoder.process_raw(chunk, False, False)\n",
        "    decoder.end_utt()\n",
        "\n",
        "    hypothesis = decoder.hyp()\n",
        "    return hypothesis.hypstr\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('sphinx-streaming-stt: \"{}\"'.format(\n",
        "        sphinx_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx-streaming-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx-streaming-stt: \"why should one hold on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx-streaming-stt: \"your paris sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZEgZKG5cWg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Mozilla DeepSpeech\n",
        "\n",
        "Mozilla released [DeepSpeech 0.6](https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/) software package in December 2019 with [APIs](https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.0) in C, Java, .NET, [Python](https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html), and JavaScript, including support for TensorFlow Lite models for use on edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilmp9i-ql7V1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install DeepSpeech**\n",
        "\n",
        "You can install DeepSpeech with pip (make it deepspeech-gpu==0.6.0 if you are using GPU in colab runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbWPbs_27f3Y",
        "outputId": "583f2b3c-cdea-4027-b859-13118fc4b538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!pip install deepspeech==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f4/1ef0397097e8a8bbb7e24caabecbdb226b4e027e5018e9353ef65af14672/deepspeech-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (9.6MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech==0.6.0) (1.17.5)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIe7haLO7yo4"
      },
      "source": [
        "2. **Download and unzip models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT-n1jLj8Ff4",
        "outputId": "eb58aab5-aafe-4d3c-97dc-58ad4fd7e6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz\n",
        "!tar -xvzf deepspeech-0.6.0-models.tar.gz\n",
        "!ls -l ./deepspeech-0.6.0-models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   620    0   620    0     0   2857      0 --:--:-- --:--:-- --:--:--  2857\n",
            "100 1172M  100 1172M    0     0  48.9M      0  0:00:23  0:00:23 --:--:-- 56.8M\n",
            "deepspeech-0.6.0-models/\n",
            "deepspeech-0.6.0-models/lm.binary\n",
            "deepspeech-0.6.0-models/output_graph.pbmm\n",
            "deepspeech-0.6.0-models/output_graph.pb\n",
            "deepspeech-0.6.0-models/trie\n",
            "deepspeech-0.6.0-models/output_graph.tflite\n",
            "total 1350664\n",
            "-rw-r--r-- 1 501 staff 945699324 Dec  3 06:51 lm.binary\n",
            "-rw-r--r-- 1 501 staff 188914896 Dec  3 09:03 output_graph.pb\n",
            "-rw-r--r-- 1 501 staff 188915850 Dec  3 09:49 output_graph.pbmm\n",
            "-rw-r--r-- 1 501 staff  47335752 Dec  3 09:05 output_graph.tflite\n",
            "-rw-r--r-- 1 501 staff  12200736 Dec  3 06:51 trie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGaM4wp8Ykp"
      },
      "source": [
        "3. **Test that it all works**\n",
        "\n",
        "Examine the output of the last three commands, and you will see results *“experience proof less”*, *“why should one halt on the way”*, and *“your power is sufficient i said”* respectively. You are all set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pPnZssj8fPY",
        "outputId": "5ebaeec2-f484-4047-9766-026a3f53d730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/2830-3980-0043.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:27:46.675441: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.13s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000221s.\n",
            "Running inference.\n",
            "experience proof less\n",
            "Inference took 2.418s for 1.975s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxm5RE68zu4",
        "outputId": "84c877c7-d1fd-4bd9-ae96-56f63bf37dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/4507-16021-0012.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:27:53.427469: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.131s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000188s.\n",
            "Running inference.\n",
            "why should one halt on the way\n",
            "Inference took 2.941s for 2.735s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hq_tEFQ8254",
        "outputId": "7f4a4720-72da-442a-ea4d-d7f08a66ec0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/8455-210777-0068.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:28:00.365841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.129s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000228s.\n",
            "Running inference.\n",
            "your power is sufficient i said\n",
            "Inference took 2.839s for 2.590s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTcABJ2c9CRa"
      },
      "source": [
        "## Create model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU41WTEr9G-X",
        "outputId": "8c4f73ad-f61f-4467-a3fa-23ef5375de74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import deepspeech\n",
        "\n",
        "model_file_path = 'deepspeech-0.6.0-models/output_graph.pbmm'\n",
        "beam_width = 500\n",
        "model = deepspeech.Model(model_file_path, beam_width)\n",
        "\n",
        "# Add language model for better accuracy\n",
        "lm_file_path = 'deepspeech-0.6.0-models/lm.binary'\n",
        "trie_file_path = 'deepspeech-0.6.0-models/trie'\n",
        "lm_alpha = 0.75\n",
        "lm_beta = 1.85\n",
        "model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4wl_9P9ilW"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaKt_rm9wY_",
        "outputId": "8bc1dc02-3c8b-4a66-ddb4-b61f362167e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def deepspeech_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    data16 = np.frombuffer(buffer, dtype=np.int16)\n",
        "    return model.stt(data16)\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-batch-stt: \"{}\"'.format(\n",
        "        deepspeech_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "deepspeech-batch-stt: \"experience proof less\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "deepspeech-batch-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "deepspeech-batch-stt: \"your power is sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3jT8NR-qGb"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU7lHQ2A-svH",
        "outputId": "8fc02288-a1a9-4709-ef25-bd42c4c99bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "def deepspeech_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "\n",
        "    # Create stream\n",
        "    context = model.createStream()\n",
        "\n",
        "    text = ''\n",
        "    for chunk in audio_generator:\n",
        "        data16 = np.frombuffer(chunk, dtype=np.int16)\n",
        "        # feed stream of chunks\n",
        "        model.feedAudioContent(context, data16)\n",
        "        interim_text = model.intermediateDecode(context)\n",
        "        if interim_text != text:\n",
        "            text = interim_text\n",
        "            print('inetrim text: {}'.format(text))\n",
        "\n",
        "    # get final resut and close stream\n",
        "    text = model.finishStream(context)\n",
        "    return text\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-streaming-stt: \"{}\"'.format(\n",
        "        deepspeech_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "inetrim text: i\n",
            "inetrim text: e\n",
            "inetrim text: experi en\n",
            "inetrim text: experience pro\n",
            "inetrim text: experience proof les\n",
            "deepspeech-streaming-stt: \"experience proof less\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "inetrim text: i\n",
            "inetrim text: why shou\n",
            "inetrim text: why should one\n",
            "inetrim text: why should one haul\n",
            "inetrim text: why should one halt \n",
            "inetrim text: why should one halt on the \n",
            "deepspeech-streaming-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "inetrim text: i\n",
            "inetrim text: your p\n",
            "inetrim text: your power is\n",
            "inetrim text: your power is suffi\n",
            "inetrim text: your power is sufficient i\n",
            "inetrim text: your power is sufficient i said\n",
            "deepspeech-streaming-stt: \"your power is sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqlb4wEcdOx"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# SpeechRecognition Package\n",
        "\n",
        "The [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) package provide a nice abstraction over several solutions. In this notebook we explore using CMU Sphinx (i.e. model running locally on the machine), and Google (i.e. service accessed over the network/cloud), but both through SpeechRecognition package APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpxAVH5OmPtn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We need to install SpeechRecognition and pocketsphinx python packages, and download some files to test these APIs.\n",
        "\n",
        "1. **Install SpeechRecognition py package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0rokUuby2i",
        "outputId": "c0d99348-92e9-49f7-edf0-20493983a1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip3 install SpeechRecognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 92kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjNf3GoTU1l"
      },
      "source": [
        "Pocketsphinx has already been installed in earlier sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIB_P7CXey4"
      },
      "source": [
        "## Batch API\n",
        "\n",
        "SpeechRecognition has only batch API. First step to create an audio record, eithher from a file or from mic, and the second step is to call `recognize_<speech engine name>` function. It currently has APIs for [CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit](https://github.com/Uberi/speech_recognition)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aia5lFgb-vV",
        "outputId": "bdf84ea8-98f0-43b9-e5f5-305c9745795e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "import speech_recognition as sr\n",
        "from enum import Enum, unique\n",
        "\n",
        "@unique\n",
        "class ASREngine(Enum):\n",
        "    sphinx = 0\n",
        "    google = 1\n",
        "\n",
        "def speech_to_text(filename: str, engine: ASREngine, language: str, show_all: bool = False) -> str:\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(filename) as source:\n",
        "        audio = r.record(source)\n",
        "\n",
        "    asr_functions = {\n",
        "        ASREngine.sphinx: r.recognize_sphinx,\n",
        "        ASREngine.google: r.recognize_google,\n",
        "    }\n",
        "\n",
        "    response = asr_functions[engine](audio, language=language, show_all=show_all)\n",
        "    return response\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    filename = t['filename']\n",
        "    text = t['text']\n",
        "    lang = t['lang']\n",
        "\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(filename, text))\n",
        "    for asr_engine in ASREngine:\n",
        "        try:\n",
        "            response = speech_to_text(filename, asr_engine, language=lang)\n",
        "            print('{0}: \"{1}\"'.format(asr_engine.name, response))\n",
        "        except sr.UnknownValueError:\n",
        "            print('{0} could not understand audio'.format(asr_engine.name))\n",
        "        except sr.RequestError as e:\n",
        "            print('{0} error: {0}'.format(asr_engine.name, e))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx: \"experience proves that\"\n",
            "google: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx: \"why should one hold on the way\"\n",
            "google: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx: \"your paris official said\"\n",
            "google: \"your power is sufficient I said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66lLoLCaL_nE"
      },
      "source": [
        "### API for other providers\n",
        "\n",
        "For other speech recognition providers, you will need to create API credentials, which you have to pass to `recognize_<speech engine name>` function, you can checkout [this example](https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py).\n",
        "\n",
        "It also has a nice abstraction for Microphone, implemented over PyAudio/PortAudio. Here is an example to capture input from mic in [batch](https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py) and continously in [background](https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTfKgcgF0uzz"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "This note covers various available speech recognition:\n",
        "\n",
        "- services: Google, Azure, Watson\n",
        "- software: CMU Sphinx, Mozilla DeepSpeech\n",
        "\n",
        "All of these have two kind of Speech-to-Text APIs:\n",
        "\n",
        "- batch: the audio data is fed in one go\n",
        "- streaming: the audio data is fed in chunks (very useful for transcribing microphone input)\n",
        "\n",
        "The Python SpeechRecognition package provides abstraction over several speech recognition services and softwares.\n",
        "\n",
        "I hope to include following in future:\n",
        "\n",
        "- services: [Amazon Transcribe](https://aws.amazon.com/transcribe/), and [Nuance](https://nuancedev.github.io/samples/http/python/)\n",
        "- software: [Kaldi](https://pykaldi.github.io/), and [Facebook wav2letter](https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/)\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "<p>Copyright &copy 2020 <a href=\"https://www.linkedin.com/in/scgupta\">Satish Chandra Gupta</a>.</p>\n",
        "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\" align=\"left\"/> <p>&nbsp;<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\">CC BY-NC-SA 4.0 International</a> License.</p>"
      ]
    }
  ]
}
